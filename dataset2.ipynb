{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQz9TU6g9LZj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nffn1c-U-Oo8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from tabulate import tabulate\n",
        "from google.colab import files\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,make_scorer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcDrPZ56-dsu"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Autism_Prediction/dataset2.csv')\n",
        "print(df.head())\n",
        "\n",
        "# bootstrap_sample = df.sample(n=500, replace=True)\n",
        "# augmented_df = pd.concat([df, bootstrap_sample], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j23iDKUm-vXP"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqBiqal-_HO5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUjEYXqa_Kwg"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT2XxTyKyYLZ"
      },
      "outputs": [],
      "source": [
        "plt.pie(df['Class'].value_counts().values, autopct='%1.1f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq0VWf48yq01"
      },
      "outputs": [],
      "source": [
        "#Categorizing the columns in the dataset based on their datatype\n",
        "ints = []\n",
        "objects = []\n",
        "floats = []\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == int:\n",
        "        ints.append(col)\n",
        "    elif df[col].dtype == object:\n",
        "        objects.append(col)\n",
        "    else:\n",
        "        floats.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6cJq-N9zNrR"
      },
      "outputs": [],
      "source": [
        "ints.remove('Class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqK-zYGGzad6"
      },
      "outputs": [],
      "source": [
        "#visulaizing how each categorical variable in the ints class affect the target\n",
        "plt.subplots(figsize=(15, 15))\n",
        "rows = len(ints) // 3 + 1  # Calculate the number of rows dynamically\n",
        "cols = min(len(ints), 3)  # Set the number of columns to maximum 3\n",
        "for i, col in enumerate(ints):\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    sb.countplot(data=df, x=col, hue='Class')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odNMgSy91WDU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the numerical columns and check for any skewness in the data\n",
        "numerical_cols = df.select_dtypes(include=['int64']).columns\n",
        "\n",
        "plt.subplots(figsize=(15,5))\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "  plt.subplot(1, len(numerical_cols), i+1)\n",
        "  sb.distplot(df[col])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age']=df['Age'].apply(lambda x:np.log(x))"
      ],
      "metadata": {
        "id": "XWAXpVY4nmfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Pnnc6F22Pv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64']).columns\n",
        "\n",
        "plt.subplots(figsize=(15,5))\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "  plt.subplot(1, len(numerical_cols), i+1)\n",
        "  sb.boxplot(df[col])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-lCrDgNAe-t"
      },
      "outputs": [],
      "source": [
        "#Checking for highly correlated features to remove as they don't help in detecting useful patterns in the data:\n",
        "def encode_labels(data):\n",
        "    for col in data.columns:\n",
        "\n",
        "      # Here we will check if datatype\n",
        "      # is object then we will encode it\n",
        "      if data[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "    return data\n",
        "\n",
        "df = encode_labels(df)\n",
        "\n",
        "# Making a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sb.heatmap(df.corr() > 0.8, annot=True, cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvro1pjFwq-P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming screening_score and class are columns in your DataFrame\n",
        "screening_score = df['Screening Score']\n",
        "class_variable = df['Class']\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = pearsonr(screening_score, class_variable)\n",
        "\n",
        "print(\"Pearson correlation coefficient:\", correlation_coefficient)\n",
        "print(\"P-value:\", p_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAjl8md5BgkG"
      },
      "outputs": [],
      "source": [
        "#removing unnecessary or harmful data and determining the labels and target:\n",
        "removal=['Screening Score','Who is completing the test','Region','Family member with ASD history']\n",
        "features=df.drop(removal+['Class'],axis=1)\n",
        "target=df['Class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKanW8SZDeKy"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.3, random_state=10)\n",
        "\n",
        "# Since the data was highly imbalanced we will balance it by  adding repitive rows of minority classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G52HoRbMD_8c"
      },
      "outputs": [],
      "source": [
        "#Normalizing the data for stable and fast training:\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform both the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXAgAcb0w-iL"
      },
      "outputs": [],
      "source": [
        "dt=DecisionTreeClassifier(max_depth=3,min_samples_leaf=10,random_state=1, criterion='entropy')\n",
        "rf=RandomForestClassifier(n_estimators=100,max_depth=10,max_features='sqrt')\n",
        "gb=GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,max_depth=3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\"SVM \":SVC(kernel='rbf'),\"Gradient Boosting\" :gb,\"KNN\": KNeighborsClassifier(),\"Random Forest\": rf, \"Decision Tree\": dt, \"Naive Bayes\": GaussianNB()}\n",
        "metrics = [\n",
        "    {'name': 'accuracy', 'metric': make_scorer(accuracy_score)},\n",
        "    {'name': 'precision', 'metric': make_scorer(precision_score)},\n",
        "    {'name': 'recall', 'metric': make_scorer(recall_score)},\n",
        "    {'name': 'f1_score', 'metric': make_scorer(f1_score)}\n",
        "]\n",
        "results_table=[]\n",
        "for i in range(len(list(models))):\n",
        "  model=list(models.values())[i]\n",
        "  model.fit(X_train_resampled,Y_train_resampled) #Training the model\n",
        "\n",
        "  #Making predictions\n",
        "  Y_train_pred=model.predict(X_train_resampled)\n",
        "  Y_test_pred=model.predict(X_test)\n",
        "\n",
        "  #Testing set performance\n",
        "  model_test_accuracy =accuracy_score(Y_test,Y_test_pred)\n",
        "  model_test_f1=f1_score(Y_test,Y_test_pred,average='weighted')\n",
        "  model_test_precision=precision_score(Y_test,Y_test_pred)\n",
        "  model_test_recall=recall_score(Y_test,Y_test_pred)\n",
        "\n",
        "  print(list(models.keys())[i])\n",
        "\n",
        "  #Testing set results\n",
        "  print('Model performance for testing set')\n",
        "  print('-Accuracy: {:.4f}'.format(model_test_accuracy))\n",
        "  print('-Precision: {:.4f}'.format(model_test_precision))\n",
        "  print('-recall: {:.4f}'.format(model_test_recall))\n",
        "  print('-f1_score: {:.4f}'.format(model_test_f1))\n",
        "  print(\"$$$$$$$$$$$$$$$$$$$\")\n",
        "  results_table.append([list(models.keys())[i], model_test_accuracy, model_test_precision, model_test_recall,\n",
        "                          model_test_f1])\n",
        "\n",
        "  print('\\n ')\n"
      ],
      "metadata": {
        "id": "nComq5MejaWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extracting data for plotting\n",
        "model_names = [result[0] for result in results_table]\n",
        "accuracy_scores = [result[1] for result in results_table]\n",
        "precision_scores = [result[2] for result in results_table]\n",
        "recall_scores = [result[3] for result in results_table]\n",
        "f1_scores = [result[4] for result in results_table]\n",
        "\n",
        "# Creating a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(model_names))\n",
        "\n",
        "bar1 = ax.bar(index, accuracy_scores, bar_width, label='Accuracy')\n",
        "bar2 = ax.bar(index + bar_width, precision_scores, bar_width, label='Precision')\n",
        "bar3 = ax.bar(index + 2 * bar_width, recall_scores, bar_width, label='Recall')\n",
        "bar4 = ax.bar(index + 3 * bar_width, f1_scores, bar_width, label='F1 Score')\n",
        "\n",
        "# Adding labels\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Performance Comparison of Models')\n",
        "ax.set_xticks(index + 1.5 * bar_width)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "\n",
        "# Display the chart\n",
        "plt.savefig('performance_chart2.png')\n",
        "files.download('performance_chart2.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0BKghWxKufnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for each model\n",
        "param_grid_dt = {'max_depth': [None, 10, 20, 30, 40, 50]}\n",
        "param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30]}\n",
        "param_grid_svm = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
        "param_grid_nb = {}  # No hyperparameters for Naive Bayes\n",
        "param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}\n",
        "param_grid_gb={'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]}\n",
        "\n",
        "# Create a dictionary of models and their respective parameter grids\n",
        "models_grid = {\n",
        "    \"SVM\": (SVC(kernel='rbf'), param_grid_svm),\n",
        "    \"Gradient Boosting\": (gb,param_grid_gb),\n",
        "    \"KNN\": (KNeighborsClassifier(), param_grid_knn),\n",
        "    \"Random Forest\": (rf, param_grid_rf),\n",
        "    \"Decision Tree\": (dt, param_grid_dt),\n",
        "    \"Naive Bayes\": (GaussianNB(), param_grid_nb)\n",
        "}\n",
        "\n",
        "metrics = [\n",
        "    {'name': 'accuracy', 'metric': make_scorer(accuracy_score)},\n",
        "    {'name': 'precision', 'metric': make_scorer(precision_score)},\n",
        "    {'name': 'recall', 'metric': make_scorer(recall_score)},\n",
        "    {'name': 'f1_score', 'metric': make_scorer(f1_score)}\n",
        "]\n",
        "\n",
        "#Initializing lists to store the results to represent in a chart\n",
        "cv_model_names = [model_name for model_name, _ in models_grid.items()]\n",
        "cv_model_names = []\n",
        "cv_accuracy_scores = []\n",
        "cv_precision_scores = []\n",
        "cv_recall_scores = []\n",
        "cv_f1_scores = []\n",
        "results=[]\n",
        "# Perform Grid Search with Cross-Validation\n",
        "for model_name, (model, param_grid) in models_grid.items():\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train_resampled, Y_train_resampled)\n",
        "\n",
        "    # Get the best parameters and the best model\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    Y_test_pred = best_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
        "    test_precision = precision_score(Y_test, Y_test_pred)\n",
        "    test_f1 = f1_score(Y_test, Y_test_pred)\n",
        "    test_recall = recall_score(Y_test, Y_test_pred)\n",
        "\n",
        "    print(\"Performance on the test set:\")\n",
        "    print('- Accuracy: {:.4f}'.format(test_accuracy))\n",
        "    print('- Precision: {:.4f}'.format(test_precision))\n",
        "    print('- Recall: {:.4f}'.format(test_recall))\n",
        "    print('- F1 Score: {:.4f}'.format(test_f1))\n",
        "    print(\"-------------------------------\")\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "    for metric in metrics:\n",
        "      model,_=models_grid[model_name]\n",
        "      scores = cross_val_score(model, X_train_resampled, Y_train_resampled, cv=10, scoring=metric['metric'])\n",
        "      print('Cross-validation ' + metric['name'] + ' average scores for ' + model.__class__.__name__ + ':', scores.mean())\n",
        "      #print('Average cross-validation score: {:.4f}'.format(scores.mean()))\n",
        "      print('----------------------------------------')\n",
        "\n",
        "    print('\\n ')"
      ],
      "metadata": {
        "id": "2g_VRB-llC_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data for plotting\n",
        "model_names = [result[0] for result in results_table]\n",
        "accuracy_scores = [result[1] for result in results_table]\n",
        "precision_scores = [result[2] for result in results_table]\n",
        "recall_scores = [result[3] for result in results_table]\n",
        "f1_scores = [result[4] for result in results_table]\n",
        "\n",
        "# Creating a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(model_names))\n",
        "\n",
        "bar1 = ax.bar(index, accuracy_scores, bar_width, label='Accuracy')\n",
        "bar2 = ax.bar(index + bar_width, precision_scores, bar_width, label='Precision')\n",
        "bar3 = ax.bar(index + 2 * bar_width, recall_scores, bar_width, label='Recall')\n",
        "bar4 = ax.bar(index + 3 * bar_width, f1_scores, bar_width, label='F1 Score')\n",
        "\n",
        "# Adding labels\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Performance Comparison of Models')\n",
        "ax.set_xticks(index + 1.5 * bar_width)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "\n",
        "# Display the chart\n",
        "plt.savefig('performance_chart_grid_search2.png')\n",
        "files.download('performance_chart_grid_search2.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "plnMXQB7vNO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before SMOTE\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(Y_train.value_counts(), labels=Y_train.value_counts().index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution Before SMOTE')\n",
        "plt.show()\n",
        "\n",
        "# After SMOTE\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(pd.Series(Y_train_resampled).value_counts(), labels=pd.Series(Y_train_resampled).value_counts().index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution After SMOTE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EJ31lnKRTRyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}